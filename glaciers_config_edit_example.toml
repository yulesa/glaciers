# Glaciers config file:
# This file was pre loaded with all the default configurations for Glaciers. 
# You can use a file only with the parameters you want to change, don't need to include all the parameters.
# You can change the configurations by calling the set_config_toml(config_file_path) function 
# or using the set_config(config_key, config_value) function.

[glaciers]
# prefered_dataframe_type: prefered dataframe type for the outputs of the glaciers functions, allowed values = ["polars", "pandas"]
prefered_dataframe_type = "polars"  
# Use hex string encoding for binary columns in the unnesting function
unnesting_hex_string_encoding = false

# configuration for the CLI main component
[main]
# If using a ABI database with both events and functions, set the following paths to the same file.
# Path to the file containing the events ABI (Application Binary Interface) signatures, it can be a csv or parquet file.
events_abi_db_file_path = "ABIs/ethereum__events__abis.parquet"
# Path to the file containing the functions ABI (Application Binary Interface) signatures, it can be a csv or parquet file.
functions_abi_db_file_path = "ABIs/ethereum__functions__abis.parquet"
# Directory containing individual ABI JSON files to be read
abi_folder_path = "ABIs/abi_database"
# Directory where raw blockchain logs are stored
raw_logs_folder_path = "data/logs"
# Directory where raw blockchain traces are stored
raw_traces_folder_path = "data/traces"

# Configuration for the ABI reader component
[abi_reader]
# Use hex string encoding for binary columns for the abi_df
output_hex_string_encoding = false
# Fields used to uniquely identify each ABI item. Allowed_keys = ["hash", "full_signature", "address"]
unique_key = ["hash", "full_signature", "address"]

[decoder]
# algorithm: algorithm to use for matching logs/traces to ABI signatures, allowed values = 
#   - "hash_address": match logs/traces to ABI signatures using both hash and address, only contracts with ABI in the ABI DB will be matched
#   - "hash": match logs/traces to ABI signatures by hash, for contracts without ABIs in the ABI DB, the most frequent signature in the ABI DB will be matched
algorithm = "hash_address"
# Use hex string encoding for binary columns in the outputing dataframes
output_hex_string_encoding = false
# output_file_format: format of the output saved file, allowed values = ["csv", "parquet"]
output_file_format = "parquet"
# Maximum number of threads spwaned which will process each log files in parallel.
# Each thread will process one log file, and can break it into chunks.
# Only used when processing multiple log files in folder.
max_concurrent_files_decoding = 16
# Maximum number of threads spwaned when processing each file.
# Each thread will process a decoded_chunk_size of the log file.
# Only used when processing larger log files (bigger than decoded_chunk_size).
max_chunk_threads_per_file = 16
# Number of log entries to process in each chunk.
decoded_chunk_size = 500_000

# Settings for the log decoder component
[log_decoder]
# Schema in the raw logs input dataframe
# alias: name of the column in the input dataframe
schema.alias = {topic0 = "topic0", topic1 = "topic1", topic2 = "topic2", topic3 = "topic3", data = "data", address = "address"}
# datatype: type of the column in the input dataframe, allowed values = ["Binary", "HexString"]
schema.datatype = {topic0 = "Binary", topic1 = "Binary", topic2 = "Binary", topic3 = "Binary", data = "Binary", address = "Binary"}

# Settings for the trace decoder component
[trace_decoder]
# Schema in the raw traces input dataframe
# alias: name of the column in the input dataframe
schema.alias = {selector = "selector", action_input = "action_input", result_output = "result_output", action_to = "action_to"}
# datatype: type of the column in the input dataframe, allowed values = ["Binary", "HexString"]
schema.datatype = {selector = "Binary", action_input = "Binary", result_output = "Binary", action_to = "Binary"}