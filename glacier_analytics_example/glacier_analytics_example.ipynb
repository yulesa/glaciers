{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Glaciers for Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glaciers is a robust tool for decoding Ethereum logs and transforming raw blockchain data into insights. In this post, we’ll explore how to use Glaciers to decode logs and analyze data. For the example, we will gather and decode all the data required to replicate the [UniswapV2 Analytics](https://app.uniswap.org/explore/pools/ethereum/0x8aE720a71622e824F576b4A8C03031066548A3B1) page. It demonstrates fetching raw logs, decoding them, and generating analytics using Python.\n",
    "\n",
    "We chose this approach because the amount of data to process this specific pool is small enough to allow runtime requests and execution in a simple example. For production use, Glaciers offers significant advantages by decoupling the data flow processes. Indexing can be executed separately for multiple contracts simultaneously, independently handling errors and backfilling when needed. Decoding can then occur in batches, leveraging all available ABIs at once. Finally, the decoded data can serve as a foundation for exploration and further transformations (Analytics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can install the necessary libraries using pip\n",
    "# `pip install polars glaciers hypersync requests toml pandas nbformat plotly`, or do `uv sync` if you are using uv.\n",
    "\n",
    "# Import the necessary libraries\n",
    "import polars as pl\n",
    "import glaciers as gl\n",
    "import requests\n",
    "import hypersync\n",
    "from hypersync import BlockField, LogField\n",
    "import asyncio\n",
    "import toml\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Raw Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glaciers assumes you already have your raw logs indexed. Many tools exist to index EVM data, but for this example, we are using [Envio’s HyperSync](https://docs.envio.dev/docs/HyperSync/overview).\n",
    "\n",
    "HyperSync is a high-performance alternative to JSON-RPC, avoiding the limitations of public free RPC providers like rate limits and error handling. HyperSync allows you to filter blocks, logs, transactions, and traces using queries.\n",
    "\n",
    "In this example, we query only logs from the `uniV2RAIETH` pool contract starting at block `11,848,623` (the deployment block). Yet, nothing prevents fetching data for multiple (or maybe all) contracts at the same time.\n",
    "\n",
    "Additionally, we extract block timestamps and join them with the logs, creating a final table where each log includes a timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the contract address, uniV2RAIETH pool contract.\n",
    "contract = [\"0x8aE720a71622e824F576b4A8C03031066548A3B1\"]\n",
    "# Create hypersync client using the mainnet hypersync endpoint (default)\n",
    "client = hypersync.HypersyncClient(hypersync.ClientConfig())\n",
    "# Define the query\n",
    "query = hypersync.Query(\n",
    "    from_block=11_848_623,\n",
    "    logs=[\n",
    "        hypersync.LogSelection(\n",
    "            address=contract,\n",
    "        )\n",
    "    ],\n",
    "    field_selection=hypersync.FieldSelection(\n",
    "        block=[\n",
    "            BlockField.NUMBER,\n",
    "            BlockField.TIMESTAMP\n",
    "        ],\n",
    "        log=[\n",
    "            LogField.LOG_INDEX,\n",
    "            LogField.TRANSACTION_HASH,\n",
    "            LogField.ADDRESS,\n",
    "            LogField.TOPIC0,\n",
    "            LogField.TOPIC1,\n",
    "            LogField.TOPIC2, \n",
    "            LogField.TOPIC3,\n",
    "            LogField.DATA,\n",
    "            LogField.BLOCK_NUMBER,\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "# Collect the logs and blocks\n",
    "await client.collect_parquet(query=query, path=\"data\", config=hypersync.ClientConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the logs and blocks\n",
    "ethereum__logs__uniV2RAIETH = pl.read_parquet(\"data/logs.parquet\")\n",
    "ethereum__blocks__mainnet = pl.read_parquet(\"data/blocks.parquet\")\n",
    "# Join the logs and blocks\n",
    "ethereum__logs__uniV2RAIETH = ethereum__logs__uniV2RAIETH.join(\n",
    "    ethereum__blocks__mainnet,\n",
    "    left_on=[\"block_number\"],\n",
    "    right_on=[\"number\"],\n",
    "    how=\"inner\"\n",
    "# Convert to timestamp and rename the timestamp to block_timestamp\n",
    ").with_columns(\n",
    "    (pl.col(\"timestamp\").bin.encode('hex').str.to_integer(base=16)*1000).cast(pl.Datetime(time_unit=\"ms\")).alias(\"block_timestamp\")\n",
    ").drop([\"timestamp\"])\n",
    "# Write the logs to a parquet file\n",
    "ethereum__logs__uniV2RAIETH.write_parquet(\"data/ethereum__logs__uniV2RAIETH.parquet\")\n",
    "ethereum__logs__uniV2RAIETH.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Glaciers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o ensure smooth operation, Glaciers rely on configuration parameters. These parameters can be modified using two methods:\n",
    "\n",
    "1. **TOML Configuration File:** Using `set_config_toml` specifies all the changes in a TOML file.\n",
    "2. **Programmatic Configuration:** Use the `set_config` method to update specific fields.\n",
    "\n",
    "The `get_config` method retrieves all current configurations, enabling easy inspection. Configurations are key-value pairs in a dictionaries format. Deeper-level keys are accessed via dot notation. Key configurations include:\n",
    "\n",
    "- Schema for raw input logs\n",
    "- Output formats\n",
    "- Parallel thread parameters for decoding files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a toml config file, to set the config.\n",
    "gl.set_config_toml(\"example_glaciers_config.toml\")\n",
    "# Set the config fields instead of using a toml file. It has the same effect as above.\n",
    "gl.set_config(field=\"decoder.output_hex_string_encoding\", value=True)\n",
    "gl.set_config(field=\"abi_reader.output_hex_string_encoding\", value=True)\n",
    "# Print the config\n",
    "config = toml.loads(gl.get_config())\n",
    "print(config[\"abi_reader\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an ABI dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in decoding is creating an ABI DataFrame. This table consolidates data from multiple ABIs and serves as input for the decoding process. Typically, each contract address corresponds to an ABI file. Glaciers provides functions to aggregate:\n",
    "\n",
    "- Multiple ABI files from a folder\n",
    "- A single ABI (our simplified case)\n",
    "- Manually inputted ABI data\n",
    "\n",
    "Since our example is only one ABI, we can copy it from [here](https://etherscan.io/address/0x8aE720a71622e824F576b4A8C03031066548A3B1#code). The generated file contains essential information, including hashes and full signatures for events and functions, all compiled into one reference table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read one ABI file\n",
    "UniswapV2PairABI = gl.read_new_abi_file(\"data/0x8ae720a71622e824f576b4a8c03031066548a3b1.json\")\n",
    "# Write the ABI to a parquet file\n",
    "UniswapV2PairABI.write_parquet(\"data/UniswapV2PairABI.parquet\")\n",
    "UniswapV2PairABI.tail(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ABI DataFrame ready, the real action begins. Raw event data is matched with ABI items, and each row is decoded using a User Defined Function (UDF). This process adds decoded columns to the schema, enriching the raw logs with details like:\n",
    "\n",
    "- Event `name`\n",
    "- Event `full_signature`\n",
    "- Event `anonymous` status\n",
    "- Decoded values (`event_values`, `event_keys`, `event_json`)\n",
    "\n",
    "Glaciers supports batch decoding for multiple files, single files, or data frames, and the entire process can be executed with a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the logs   \n",
    "decoded_logs_df = gl.decode_file(decoder_type=\"log\", file_path=\"data/ethereum__logs__uniV2RAIETH.parquet\", abi_db_path=\"data/UniswapV2PairABI.parquet\")\n",
    "# Drop the columns that are not needed\n",
    "decoded_logs_df = decoded_logs_df.drop([\"topic0\", \"topic1\", \"topic2\", \"topic3\", \"data\", \"full_signature\", \"anonymous\"])\n",
    "decoded_logs_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring and Filtering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoded logs provide a wealth of data for analytics. The resulting table includes all contracts and events, with event keys and values nested in JSON objects for flexibility.\n",
    "\n",
    "When decoding a single contract, as in this example, the data is somehow straightforward. However, decoding multiple or all contracts introduces greater exploration potential. You can inspect different contracts or event fields, and determine what information is valuable for analytics. This is similar to selecting tables in Dune Analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show all events names and keys\n",
    "decoded_logs_df.group_by([\"name\", \"event_keys\"]).len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoded Logs table contains all contracts and events together. This constrain that `event_keys` and `event_values` be nested within JSON objects. By listing all the keys, it’s easy to filter and unnest the values into individual tables.\n",
    "\n",
    "- **Filtering:** Filter tables by the required contracts and events. Filtering can be done on-demand, narrowing data to what you need.\n",
    "- **Unnesting Values:** After filtering, unnest the values into individual columns since all rows now share the same structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mint_df = decoded_logs_df\\\n",
    "    .filter(pl.col(\"name\") == \"Mint\")\\\n",
    "    .filter(pl.col(\"address\") == \"0x8ae720a71622e824f576b4a8c03031066548a3b1\")\\\n",
    "    .with_columns(\n",
    "    pl.col(\"event_values\").str.json_decode().list.get(0).alias(\"sender\"),\n",
    "    pl.col(\"event_values\").str.json_decode().list.get(1).cast(pl.Float64).alias(\"amount0\"), #amount0 is the amount of RAI\n",
    "    pl.col(\"event_values\").str.json_decode().list.get(2).cast(pl.Float64).alias(\"amount1\"), #amount1 is the amount of ETH\n",
    "    )\\\n",
    "    .drop([\"event_values\", \"event_keys\", \"event_json\"])\n",
    "swap_df = decoded_logs_df\\\n",
    "    .filter(pl.col(\"name\") == \"Swap\")\\\n",
    "    .filter(pl.col(\"address\") == \"0x8ae720a71622e824f576b4a8c03031066548a3b1\")\\\n",
    "    .with_columns(\n",
    "    pl.col(\"event_values\").str.json_decode().list.get(0).alias(\"sender\"),\n",
    "    pl.col(\"event_values\").str.json_decode().list.get(1).alias(\"to\"),\n",
    "    pl.col(\"event_values\").str.json_decode().list.get(2).cast(pl.Float64).alias(\"amount0In\"), #amount0In is the amount of RAI\n",
    "    pl.col(\"event_values\").str.json_decode().list.get(3).cast(pl.Float64).alias(\"amount1In\"), #amount1In is the amount of ETH   \n",
    "    pl.col(\"event_values\").str.json_decode().list.get(4).cast(pl.Float64).alias(\"amount0Out\"), #amount0Out is the amount of RAI\n",
    "    pl.col(\"event_values\").str.json_decode().list.get(5).cast(pl.Float64).alias(\"amount1Out\"), #amount1Out is the amount of ETH\n",
    "    )\\\n",
    "    .drop([\"event_values\", \"event_keys\", \"event_json\"])\n",
    "burn_df = decoded_logs_df\\\n",
    "    .filter(pl.col(\"name\") == \"Burn\")\\\n",
    "    .filter(pl.col(\"address\") == \"0x8ae720a71622e824f576b4a8c03031066548a3b1\")\\\n",
    "    .with_columns(\n",
    "    pl.col(\"event_values\").str.json_decode().list.get(0).alias(\"sender\"),\n",
    "    pl.col(\"event_values\").str.json_decode().list.get(1).alias(\"to\"),\n",
    "    pl.col(\"event_values\").str.json_decode().list.get(2).cast(pl.Float64).alias(\"amount0\"), #amount0 is the amount of RAI\n",
    "    pl.col(\"event_values\").str.json_decode().list.get(3).cast(pl.Float64).alias(\"amount1\"), #amount1 is the amount of ETH\n",
    "    )\\\n",
    "    .drop([\"event_values\", \"event_keys\", \"event_json\"])\n",
    "sync_df = decoded_logs_df\\\n",
    "    .filter(pl.col(\"name\") == \"Sync\")\\\n",
    "    .filter(pl.col(\"address\") == \"0x8ae720a71622e824f576b4a8c03031066548a3b1\")\\\n",
    "    .with_columns(\n",
    "    pl.col(\"event_values\").str.json_decode().list.get(0).cast(pl.Float64).alias(\"reserve0\"), #reserve0 is the amount of RAI\n",
    "    pl.col(\"event_values\").str.json_decode().list.get(1).cast(pl.Float64).alias(\"reserve1\"), #reserve1 is the amount of ETH\n",
    "    )\\\n",
    "    .drop([\"event_values\", \"event_keys\", \"event_json\"])\n",
    "transfer_df = decoded_logs_df\\\n",
    "    .filter(pl.col(\"name\") == \"Transfer\")\\\n",
    "    .filter(pl.col(\"address\") == \"0x8ae720a71622e824f576b4a8c03031066548a3b1\")\\\n",
    "    .with_columns(\n",
    "    pl.col(\"event_values\").str.json_decode().list.get(0).alias(\"from\"),\n",
    "    pl.col(\"event_values\").str.json_decode().list.get(1).alias(\"to\"),\n",
    "    pl.col(\"event_values\").str.json_decode().list.get(2).cast(pl.Float64).alias(\"value\"),\n",
    "    )\\\n",
    "    .drop([\"event_values\", \"event_keys\", \"event_json\"])\n",
    "swap_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analytics, we need additional input data: daily ETH prices. We retrieve the last 365 days of historical prices using the free CoinGecko API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the connection with a price provider.\n",
    "price_api_url = \"https://api.coingecko.com/api/v3/coins/ethereum/market_chart?vs_currency=usd&days=365&interval=daily\"\n",
    "# Define the request params.\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "# Request, receive and store the ETH price.\n",
    "eth_price = requests.get(price_api_url, headers=headers).json()\n",
    "# Convert the price to a dataframe\n",
    "eth_price_df = pl.DataFrame(\n",
    "    eth_price[\"prices\"],\n",
    "    schema=[\"datetime\", \"eth_price_usd\"],\n",
    "    orient=\"row\",\n",
    ")\n",
    "# Convert the datetime to a date type\n",
    "eth_price_df = eth_price_df.with_columns(\n",
    "    pl.col(\"datetime\").cast(pl.Int64).cast(pl.Datetime(time_unit=\"ms\")).dt.date().alias(\"datetime\")\n",
    ")\n",
    "# Write the dataframe to a parquet file\n",
    "eth_price_df.write_parquet(\"data/coingecko__eth_price.parquet\")\n",
    "eth_price_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume, liquidity and price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the individual events table to transform the data into analytics. It requires some understanding of the interworkings of the contracts we are dealing with.\n",
    "\n",
    "1. **Daily Volume:** Aggregating all swap events in the pool. Multiply the volume in ETH terms by the ETH price to calculate USD volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_df = swap_df.select(\n",
    "    pl.col(\"block_timestamp\").dt.date().alias(\"datetime\"),\n",
    "    ((pl.col(\"amount0In\")+pl.col(\"amount0Out\"))/1e18).alias(\"Vol (RAI)\"),\n",
    "    ((pl.col(\"amount1In\")+pl.col(\"amount1Out\"))/1e18).alias(\"Vol (ETH)\"),\n",
    ").group_by([\"datetime\"]).agg(\n",
    "    pl.col(\"Vol (RAI)\").sum().alias(\"Vol (RAI)\"),\n",
    "    pl.col(\"Vol (ETH)\").sum().alias(\"Vol (ETH)\"),\n",
    ").join(\n",
    "    eth_price_df, on=\"datetime\", how=\"inner\"\n",
    ").with_columns(\n",
    "    (pl.col(\"Vol (ETH)\")*pl.col(\"eth_price_usd\")).alias(\"Vol (USD)\"),\n",
    ").unpivot(\n",
    "    on=[\"Vol (USD)\", \"Vol (RAI)\", \"Vol (ETH)\"],\n",
    "    index=\"datetime\",\n",
    "    value_name=\"Volume\",\n",
    "    variable_name=\"Token\",\n",
    ").sort(\"datetime\", descending=True)\n",
    "volume_df.head(10)\n",
    "fig1 = px.bar(volume_df, x=\"datetime\", y=\"Volume\", facet_row=\"Token\", title=\"Volume\", barmode=\"group\", labels={\"datetime\":\"Time\", \"Volume\":\"\"})\n",
    "fig1.update_yaxes(matches=None)\n",
    "fig1.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Liquidity:** The sync event shows us the reserves on each side of the pool. Convert reserves in ETH terms to USD. Since UniswapV2 ensures that both sides of the pool have equal value, double the USD value of the reserve is the total liquidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liquidity_df = sync_df.select(\n",
    "    pl.col(\"block_timestamp\").dt.date().alias(\"datetime\"),\n",
    "    (pl.col(\"reserve0\")/1e18).alias(\"Reserve (RAI tokens)\"),\n",
    "    (pl.col(\"reserve1\")/1e18).alias(\"Reserve (ETH tokens)\"),\n",
    ").group_by([\"datetime\"]).agg(\n",
    "    pl.col(\"Reserve (RAI tokens)\").last().alias(\"Reserve (RAI tokens)\"),\n",
    "    pl.col(\"Reserve (ETH tokens)\").last().alias(\"Reserve (ETH tokens)\"),\n",
    ").join(\n",
    "    eth_price_df, on=\"datetime\", how=\"inner\"\n",
    ").with_columns(\n",
    "    (pl.col(\"Reserve (ETH tokens)\")*pl.col(\"eth_price_usd\")).alias(\"Liquidity (USD)\"),\n",
    "    (2*pl.col(\"Reserve (ETH tokens)\")*pl.col(\"eth_price_usd\")).alias(\"Total Liquidity (USD)\"),\n",
    ").sort(\"datetime\", descending=True)\n",
    "fig2 = px.line(liquidity_df, x=\"datetime\", y=[\"Total Liquidity (USD)\", \"Reserve (RAI tokens)\", \"Reserve (ETH tokens)\"], title=\"Liquidity\", labels={\"datetime\":\"Time\", \"value\":\"Liquidity\"})\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. **Price:** Calculate the price of RAI by dividing the USD value of the ETH side by the total tokens on the RAI side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAI_price_df = liquidity_df.with_columns(\n",
    "    (pl.col(\"Liquidity (USD)\")/pl.col(\"Reserve (RAI tokens)\")).alias(\"RAI Price\"),\n",
    ")\n",
    "fig3 = px.line(RAI_price_df, x=\"datetime\", y=\"RAI Price\", title=\"RAI Price\", labels={\"datetime\":\"Time\"})\n",
    "fig3.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
